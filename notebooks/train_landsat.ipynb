{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abab8b8e-a895-41db-9e90-a7d7f2335b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import math\n",
    "import stackstac\n",
    "import geojson\n",
    "import dask_gateway\n",
    "import planetary_computer\n",
    "import rasterio.features\n",
    "import azure.storage.blob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import rioxarray as rioxr\n",
    "import matplotlib.pyplot as plt\n",
    "import dask\n",
    "from dask.distributed import PipInstall, Lock\n",
    "#!pip install pysptools\n",
    "import pysptools.abundance_maps as amp\n",
    "from scipy.stats import mode\n",
    "from dask_gateway import GatewayCluster\n",
    "from pystac_client import Client\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e03bb46d-f096-458b-92b4-fb1d046b6e9e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_file(file):\n",
    "    with open(file) as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def load_blob_grid(blob_client):\n",
    "    return geojson.loads(blob_client.download_blob().readall())\n",
    "\n",
    "def get_tile(grid, h, v):\n",
    "    return [x for x in grid['features'] if x['properties']['h'] == h\n",
    "            and x['properties']['v'] == v][0]['geometry']\n",
    "\n",
    "def get_bbox(geometry):\n",
    "    return rasterio.features.bounds(geometry)\n",
    "\n",
    "def get_container(container, connection_string):\n",
    "    container_client = azure.storage.blob.ContainerClient.from_connection_string(\n",
    "        connection_string, container_name=container\n",
    "    )\n",
    "    return container_client\n",
    "\n",
    "def get_blob(container_client, blob_name):\n",
    "    blob_client = container_client.get_blob_client(blob_name)\n",
    "    return blob_client\n",
    "\n",
    "def get_cluster(n=20, ncore=8, memory=16):\n",
    "    gateway = dask_gateway.Gateway()\n",
    "    cluster_options = gateway.cluster_options()\n",
    "    cluster_options[\"worker_cores\"] = ncore\n",
    "    cluster_options[\"worker_memory\"] = memory\n",
    "    \n",
    "    cluster = gateway.new_cluster(cluster_options)\n",
    "    #cluster.adapt(minimum=n, maximum=400)\n",
    "    cluster.scale(n)\n",
    "    client = cluster.get_client()\n",
    "    return (cluster, client)\n",
    "\n",
    "def register_package():\n",
    "    plugin = PipInstall(packages=['pysptools', 'cvxopt'], pip_options=['--upgrade'])\n",
    "    client.register_worker_plugin(plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7daf71f4-99b2-46d2-b611-9598a0883c7a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_epsg(items):\n",
    "    epsgs = [x.properties['proj:epsg'] for x in items]\n",
    "    return 'EPSG: ' + str(mode(epsgs).mode[0]) # Might cause problems in the future\n",
    "\n",
    "def search_landsat_images(start, end, geometry, limit=1000):\n",
    "    search = catalog.search(\n",
    "        intersects = geometry,\n",
    "        datetime = start + '/' + end,\n",
    "        collections = ['landsat-c2-l2'],\n",
    "        limit = 1000,\n",
    "        query={'landsat:collection_category': {'eq': 'T1'}, \n",
    "               'eo:cloud_cover': {'lt': 90}}\n",
    "    )\n",
    "    return search.get_all_items() # use get_all_items instead. It seems that we don't need to change it to \"list\"\n",
    "    #return list(search.get_items())\n",
    "    \n",
    "def get_landsat_stack(start, end, geometry, chunksize=128):\n",
    "    items = search_landsat_images(start, end, geometry)\n",
    "    # signed_items = [planetary_computer.sign(item).to_dict() for item in items] deleted, the planetary_computer.sign() is set when initialize the catalog\n",
    "    # manually sign the images seems to causal problems in accessing/opening the Landsat images \n",
    "    bbox = get_bbox(geometry)\n",
    "    epsg = get_epsg(items)\n",
    "    \n",
    "    data = (\n",
    "        stackstac.stack(\n",
    "            items,\n",
    "            assets=['blue', 'green', 'red', 'nir08', 'swir16', 'swir22', 'qa_pixel'],\n",
    "            #assets=['SR_B2', 'SR_B3', 'SR_B4', 'SR_B5', 'SR_B6', 'SR_B7', 'QA_PIXEL'],\n",
    "            chunksize=(-1, -1, chunksize, chunksize),\n",
    "            resolution=30,\n",
    "            epsg=epsg,\n",
    "            bounds_latlon=bbox\n",
    "        )\n",
    "        .assign_coords(band=['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2', 'QA'])\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def array_to_frac_year(array, days_in_year=365.25):\n",
    "    return array.time.dt.year + array.time.dt.day / days_in_year\n",
    "\n",
    "def construct_dependents(array, days_in_year=365.25):\n",
    "    x1 = array_to_frac_year(array, days_in_year)\n",
    "    omega = 2 * math.pi\n",
    "    x2 = np.cos(x1 * omega)\n",
    "    x3 = np.sin(x1 * omega)\n",
    "    return (\n",
    "        xr.concat([x1, x2, x3], dim='x')\n",
    "        .assign_coords(x=['x1', 'x2', 'x3'])\n",
    "        .transpose(*('time', 'x'))\n",
    "    )\n",
    "\n",
    "def fnrt(M, U, X, scale=10000):\n",
    "    M2 = M.astype('int16')\n",
    "    qa = M2[:, -1]\n",
    "    good = [21824, 21952, 5440, 5504]\n",
    "    mask = np.isin(qa, good)\n",
    "    sr = (M2[:, 0:6] * 0.0000275 - 0.2) * scale\n",
    "    unmixed = amp.amaps.FCLS(sr, U)\n",
    "    unmixed[mask==0, :] = np.nan\n",
    "    \n",
    "    gv = unmixed[:, 0]\n",
    "    npv = unmixed[:, 1]\n",
    "    soil = unmixed[:, 2]\n",
    "    shade = unmixed[:, 3]\n",
    "    cloud = unmixed[:, 4]\n",
    "    \n",
    "    gv_frac = (gv / (1 - shade)) + (npv + soil)\n",
    "    mask = ((cloud < 0.2) & (shade < 1) & (gv_frac > 0)).astype('uint16')\n",
    "    ndfi = (gv / (1 - shade) - (npv + soil)) / gv_frac * scale\n",
    "    ndfi[mask==0] = np.nan\n",
    "    \n",
    "    regr = linear_model.LinearRegression()\n",
    "    y_true = ndfi[~np.isnan(ndfi)]\n",
    "    x_true = X[~np.isnan(ndfi), :]\n",
    "    try: # to prevent non data cases, if all the data in one location are excluded, the \"fit\" function will fail due to lacking data\n",
    "        lm = regr.fit(x_true, y_true) \n",
    "    except:\n",
    "        lm = regr.fit([0], [0])\n",
    "    coef = lm.coef_\n",
    "    intercept = lm.intercept_\n",
    "    y_pred = lm.predict(x_true)\n",
    "    rmse = mean_squared_error(y_true=y_true, y_pred=y_pred, squared=False)\n",
    "    \n",
    "    return np.array(\n",
    "        [intercept, coef[0], coef[1], coef[2], rmse], \n",
    "        ndmin=2, \n",
    "        dtype='float64'\n",
    "    )\n",
    "    \n",
    "def xr_fnrt(col, endmembers, scale=10000):\n",
    "    X = construct_dependents(col)\n",
    "    return (\n",
    "        xr.apply_ufunc(\n",
    "            fnrt, col,\n",
    "            input_core_dims=[['time', 'band']], \n",
    "            output_core_dims=[['time', 'fit']],\n",
    "            exclude_dims=set(('time', 'band')), \n",
    "            kwargs={'X': X, 'U': endmembers,'scale': scale},\n",
    "            dask='parallelized', \n",
    "            vectorize=True,\n",
    "            output_dtypes=[col.dtype],\n",
    "            output_sizes={'time': 1, 'fit': 5}\n",
    "        )\n",
    "        .rename({'fit': 'band'})\n",
    "        .assign_coords(band=['incpt','slope','cos','sin','rmse'])\n",
    "        .transpose(*col.dims)\n",
    "        .squeeze()\n",
    "    )\n",
    "\n",
    "def export_to_drive(img, des, driver='COG', nodata=0, dask=False, client=None):\n",
    "    dataset = (img\n",
    "               .to_dataset(dim='band')\n",
    "               .rio.write_crs(img.coords['epsg'].item())\n",
    "              )\n",
    "    \n",
    "    for data_var in dataset.data_vars:\n",
    "        dataset[data_var].rio.write_nodata(nodata, inplace=True)\n",
    "    \n",
    "    if dask:\n",
    "        dataset.rio.to_raster(des, driver=driver, tiled=True, lock=Lock('fnrtm', client=client))\n",
    "    else:\n",
    "        dataset.rio.to_raster(des, driver=driver)\n",
    "    \n",
    "def export_to_blob(img, container_client, blob, driver='GTiff', nodata=0, dask=False, client=None):\n",
    "    dataset = (img\n",
    "               .to_dataset(dim='band')\n",
    "               .rio.write_crs(img.coords['epsg'].item())\n",
    "              )\n",
    "    \n",
    "    for data_var in dataset.data_vars:\n",
    "        dataset[data_var].rio.write_nodata(nodata, inplace=True)\n",
    "    \n",
    "    with io.BytesIO() as buffer:\n",
    "        if dask:\n",
    "            dataset.rio.to_raster(buffer, driver=driver, tiled=True, lock=Lock('fnrtm', client=client)) # Don't really understand this one yet\n",
    "        else:\n",
    "            dataset.rio.to_raster(buffer, driver=driver) \n",
    "        buffer.seek(0)\n",
    "        blob_client = container_client.get_blob_client(blob)\n",
    "        blob_client.upload_blob(buffer, overwrite=True)\n",
    "\n",
    "def divide_box(bbox, scale=4):\n",
    "    def get_box(x, y, step_x, step_y):\n",
    "        return (x, y, x+step_x,y+step_y)\n",
    "    l, d, r, u = bbox \n",
    "    step_x = (r - l)/scale\n",
    "    step_y = (u - d)/scale\n",
    "    lists = [] \n",
    "    for idx in range(scale):\n",
    "        for idy in range(scale):\n",
    "            l, d, r, u = get_box(l+idx*step_x, d+idy*step_y, step_x, step_y)\n",
    "            lists.append([[[l,d],[r,d],[r,u],[l,u],[l,d]]])\n",
    "    return lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9205b490-6400-4c5e-912f-5f092c01c89a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endmembers = np.array([[500, 900, 400, 6100, 3000, 1000],\n",
    "                       [1400, 1700, 2200, 3000, 5500, 3000],\n",
    "                       [2000, 3000, 3400, 5800, 6000, 5800],\n",
    "                       [0, 0, 0, 0, 0, 0],\n",
    "                       [9000, 9600, 8000, 7800, 7200, 6500]], dtype=np.int16)\n",
    "catalog = Client.open('https://planetarycomputer.microsoft.com/api/stac/v1',modifier=planetary_computer.sign_inplace) # use modifier to automatically sign\n",
    "connection_string = read_file('/home/jovyan/fnrtm/files/connect.txt')\n",
    "container_client = get_container('misc', connection_string)\n",
    "blob_client = get_blob(container_client, 'one_degree_grid.geojson')\n",
    "training_container = get_container('training', connection_string)\n",
    "grid_blob = load_blob_grid(blob_client)\n",
    "grid_lists = []\n",
    "for i in grid_blob['features']:\n",
    "    grid_lists.append(i['properties'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fae6bf-4b17-4e61-a559-a391ca4ad731",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1dcc0c4-e3db-4a3f-a4c8-0f16a1f9116f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def boxes(x,y,scale=0.5):\n",
    "    # when scale = 0.5\n",
    "    # h from 0 to 720\n",
    "    # v from 0 to 360\n",
    "    h = (360/scale)/2 + np.floor(x/scale)\n",
    "    v = (180/scale)/2 + np.floor(y/scale)\n",
    "    def get_box(x, y, step_x, step_y):\n",
    "        return (x, y, x+step_x,y+step_y)\n",
    "    l, d, r, u = get_box(x,y,scale,scale)\n",
    "    box = [[[l,d],[r,d],[r,u],[l,u],[l,d]]]\n",
    "    return box,h.astype(int),v.astype(int)\n",
    "def geometry_from_box(bbox):\n",
    "    return geojson.loads('{\"coordinates\": %s, \"geodesic\": false, \"type\": \"Polygon\"}' % geojson.dumps(bbox))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4cac056-2e2b-4c69-91b7-e14a16cb53a0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://pccompute.westeurope.cloudapp.azure.com/compute/services/dask-gateway/clusters/prod.4f884be97231479086d9be3332d9ea87/status\n"
     ]
    }
   ],
   "source": [
    "(cluster, client) = get_cluster(20,4,4) # n=20, ncore=8, memory=16\n",
    "register_package()\n",
    "print(cluster.dashboard_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3da4c476-7501-473e-a894-843ecbe6b9c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m bbox,h,v \u001b[38;5;241m=\u001b[39m boxes(\u001b[43mx\u001b[49m,y,scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m      2\u001b[0m output_name \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFNRT_\u001b[39m\u001b[38;5;132;01m%03d\u001b[39;00m\u001b[38;5;132;01m%03d\u001b[39;00m\u001b[38;5;124m_1921.tif\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (h,v))\n\u001b[1;32m      3\u001b[0m exits \u001b[38;5;241m=\u001b[39m [blob\u001b[38;5;241m.\u001b[39mname \u001b[38;5;28;01mfor\u001b[39;00m blob \u001b[38;5;129;01min\u001b[39;00m training_container\u001b[38;5;241m.\u001b[39mlist_blobs()]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-10 18:45:17,549 - distributed.client - ERROR - Failed to reconnect to scheduler after 30.00 seconds, closing client\n"
     ]
    }
   ],
   "source": [
    "bbox,h,v = boxes(x,y,scale=0.5)\n",
    "output_name = ('FNRT_%03d%03d_1921.tif' % (h,v))\n",
    "exits = [blob.name for blob in training_container.list_blobs()]\n",
    "if output_name in exits:\n",
    "    print(output_name, 'exists')\n",
    "    continue\n",
    "print(output_name,'working')\n",
    "geometry = geometry_from_box(bbox)\n",
    "lst = get_landsat_stack('2019-01-01', '2021-12-31', geometry, chunksize)\n",
    "trained =  xr_fnrt(lst, endmembers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d963e484-905f-4056-8156-b3022b034339",
   "metadata": {},
   "source": [
    "# check if there is any unclosed clusters\n",
    "gateway = dask_gateway.Gateway()\n",
    "clusters = gateway.list_clusters()\n",
    "print(clusters)\n",
    "cluster = gateway.connect(clusters[0].name)\n",
    "client = cluster.get_client()\n",
    "print(cluster.dashboard_link)\n",
    "cluster.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f0a16b-0221-47f2-a0a0-226950039488",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57d49e5-3a70-498b-8a8c-3e5160d2a0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 32\n",
    "for x,y in zip([-67,-67,-66.5,-66.5],[-9,-8.5,-9,-8.5]):\n",
    "    bbox,h,v = boxes(x,y,scale=0.5)\n",
    "    output_name = ('FNRT_%03d%03d_1921.tif' % (h,v))\n",
    "    exits = [blob.name for blob in training_container.list_blobs()]\n",
    "    if output_name in exits:\n",
    "        print(output_name, 'exists')\n",
    "        continue\n",
    "    print(output_name,'working')\n",
    "    geometry = geometry_from_box(bbox)\n",
    "    lst = get_landsat_stack('2019-01-01', '2021-12-31', geometry, chunksize)\n",
    "    trained =  xr_fnrt(lst, endmembers)\n",
    "    export_to_blob(trained, training_container, output_name, dask=False)\n",
    "    del trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b50b26-ae59-4592-a105-b080451b6fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
